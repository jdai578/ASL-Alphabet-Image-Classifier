{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New  idea  use  uh  something  to  soften  up  hard  foods  like  pineapples   \n",
      "New  idea  use  something  soften  hard  foods  pineapples  "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "set(stopwords.words(\"english\"))\n",
    " \n",
    "example_sent = \"New idea use uh something to soften up hard foods like pineapples\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "for word in word_tokens:\n",
    "    print (word + \" \", end = \" \")\n",
    "print (\" \")\n",
    "for word in filtered_sentence:\n",
    "    print (word + \" \", end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.models import model_from_json\n",
    "#from keras.utils import np_utils\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import simplejson\n",
    "\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "tags_vocabulary = \"\"\"CC CD DT EX FW IN JJ JJR JJS LS MD NN NNS NNP NNPS \n",
    "PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH VB VBD VBG VBN VBP VBZ WDT WP WRB\"\"\"   \n",
    "\n",
    "tags_vocabulary_list = tags_vocabulary.split()\n",
    "\n",
    "#  a list, so convert the stop words to a set\n",
    "#stops_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# TODO: Turn this collection of methods into a class instead. \n",
    "\n",
    "# How many max words do we think there are in a TODO item.\n",
    "# We need this for correct padding of the sentences.\n",
    "maxlen = 8 \n",
    "_activation = 'sigmoid'\n",
    "#_activation = \n",
    "_loss = 'binary_crossentropy'\n",
    "_optimizer = 'adam'\n",
    "_metrics = 'accuracy'\n",
    "\n",
    "\n",
    "# This can also by solved by linear regression using next values. Then I can just call .predict to get teh regression value.\n",
    "# _activation = 'linear' \n",
    "# _loss = 'mse'\n",
    "# _optimizer = 'rmsprop'\n",
    "# _metrics = 'accuracy'\n",
    "# Mean squared error regression problem: https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "\n",
    "# generates n-grams\n",
    "def generate_ngrams(string, n):\n",
    "    ngram_output = ngrams(string.split(), n)\n",
    "    output_array = []\n",
    "    for i in ngram_output:\n",
    "        output_array.append(' '.join(i))\n",
    "    return output_array\n",
    "\n",
    "# This one gets the tags only without removing the stop words.\n",
    "def get_tags(text):\n",
    "    # Keep leeters and numbers only\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", text) \n",
    "    lower_case = letters_only.lower() \n",
    "    tokens = word_tokenize(lower_case)\n",
    "    return [tag[1] for tag in pos_tag(tokens)]\n",
    "\n",
    "\n",
    "def get_tags_string(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = [tag[1] for tag in pos_tag(tokens)]\n",
    "    result = \" \"\n",
    "    return result.join(tags)\n",
    "\n",
    "\n",
    "def get_tags_without_stop_words(text, stops_set):\n",
    "    # Keep leeters only\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    # Convert to lower case, split into individual words\n",
    "    word_list = letters_only.lower().split()    \n",
    "    # Remove stop words\n",
    "    filtered_words = [w for w in word_list if not w in stops_set]\n",
    "\n",
    "    text_to_tokenize = \" \"\n",
    "    text_to_tokenize = text_to_tokenize.join(filtered_words)\n",
    "    tokens = word_tokenize(text_to_tokenize)\n",
    "    return [tag[1] for tag in pos_tag(tokens)]\n",
    "\n",
    "\n",
    "def get_feature_vector(text):\n",
    "    # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    #text_tags = get_tags_without_stop_words(text, stops_set)\n",
    "    text_tags = get_tags(text)\n",
    "    vector = []\n",
    "    for tag in text_tags:\n",
    "        vector.append(tags_vocabulary_list.index(tag) + 1)\n",
    "    return vector\n",
    "# In Python, searching a set is much faster than searching\n",
    "\n",
    "\n",
    "def get_text_class(text, maxlen, model):\n",
    "    feature_vec = get_feature_vector(text)\n",
    "    predict_matrix = np.array([feature_vec])\n",
    "    test = sequence.pad_sequences(predict_matrix, maxlen=maxlen)\n",
    "    predict_val = model.predict_classes(test[0:1])\n",
    "    return predict_val[0][0]\n",
    "\n",
    "\n",
    "# Need to debug this method. It was created to print incorrect predictions in a range\n",
    "# but is not working (it can not find the feaure vector)\n",
    "def test_range(test, result, feature_vectors, sentences, model):\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    predicted_classes = model.predict_classes(test)\n",
    "    \n",
    "    for index in range(len(predicted_classes)):\n",
    "        if predicted_classes[index][0] == result[index]:\n",
    "            correct_count = correct_count + 1\n",
    "        else:\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            print (\"Looking for: \", test[index])\n",
    "            sentence_index = feature_vectors.index(test[index])\n",
    "            print (\"Incorrect: \", sentences[sentence_index])\n",
    "\n",
    "    return correct_count, incorrect_count\n",
    "\n",
    "\n",
    "def create_cnn_todo_item_recognizer():\n",
    "    max_features = len(tags_vocabulary_list) + 1\n",
    "    batch_size = 10\n",
    "    embedding_dims = 50\n",
    "    filters = 128\n",
    "    kernel_size = 2\n",
    "    hidden_dims = 250\n",
    "    epochs = 15\n",
    "    # play around wiht epochs, kernel size max 4\n",
    "\n",
    "    df = pd.read_csv('/home/t-judai/ideas_dataset.csv')\n",
    "    todo_items_sentences = df['Sentence'].tolist()\n",
    "\n",
    "    # We want to train on the tags since we want to learn on the syntax.\n",
    "    x = []\n",
    "    for sentence in todo_items_sentences:\n",
    "        x.append(get_feature_vector(sentence)) \n",
    "    y = [1] * len(x)\n",
    "\n",
    "    # Used to analize incorrect predictions\n",
    "    sentences = todo_items_sentences\n",
    "\n",
    "    # Now let's load some examples of what is not a todo Item (South park dialogs downloaded\n",
    "    # from Kaggle).\n",
    "    df = pd.read_csv('/home/t-judai/seinfeld-scripts.csv')\n",
    "    \n",
    "    # Cleaning a bit the string\n",
    "    #lines = [line.replace('\\n','') for line in df['Dialogue']]\n",
    "    lines = df['Dialogue']\n",
    "    items_to_add = len(x)\n",
    "    for line in lines:\n",
    "        line = str(line).strip()\n",
    "        if line.find(' ') > 3: # Poor man's way to know there are at least 4-5 words on the string\n",
    "            if line.find('.') > 0:\n",
    "                line = line[0:line.find('.')]\n",
    "            #line = line[0:3]\n",
    "            try:\n",
    "                x.append(get_feature_vector(line))\n",
    "                sentences.append(line)\n",
    "                items_to_add = items_to_add - 1\n",
    "                y.append(0)\n",
    "            except:\n",
    "                print ('Ignoring this line:', line)\n",
    "                continue\n",
    "        if items_to_add == 0:\n",
    "            break\n",
    "\n",
    "    # Used to analize incorrect predictions\n",
    "    feature_vectors = x\n",
    "    # At this point we have the feature vectors, and the matching sentence to do look ups.\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=33)\n",
    "\n",
    "    print(len(x_train), 'train sequences')\n",
    "    print(len(x_test), 'test sequences')\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "\n",
    "\n",
    "    # Based on https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py\n",
    "    # https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "\n",
    "    ######\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims)) #hidden_dims\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(128)) #hidden_dims\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(128)) #hidden_dims\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(64))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(32))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    #####\n",
    "\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(_activation)) \n",
    "\n",
    "    model.compile(loss = _loss,\n",
    "                  optimizer = _optimizer,\n",
    "                  metrics = [_metrics])\n",
    "\n",
    "    # Fit model on training data.\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score', score[0])\n",
    "    print('Test accuracy', score[1])\n",
    "\n",
    "\n",
    "    #  serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/home/t-judai/model_hack.json\", \"w\") as json_file:\n",
    "        json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"/home/t-judai/model_hack_weights.h5\")\n",
    "\n",
    "\n",
    "    # To predict something I can do (returns an array):\n",
    "    predicted_classes = model.predict_classes(x_test[10:20])\n",
    "\n",
    "\n",
    "    #test_string = \"\"\"Schedule the meeting with the Doctor\"\"\"\n",
    "    #feature_vec = get_feature_vector(test_string)\n",
    "    #predict_matrix = np.array([feature_vec])\n",
    "    #test = sequence.pad_sequences(predict_matrix, maxlen=maxlen)\n",
    "    #predict_val = model.predict_classes(test[0:1])\n",
    "    #print(predict_val)\n",
    "\n",
    "\n",
    "def load_cnn_todo_reco_model_from_path(model_path, weights_path):\n",
    "    print(\"Loading the model from disk...\", model_path)\n",
    "    json_file = open(model_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    print(\"Loading the weights from disk...\", weights_path)\n",
    "    loaded_model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    loaded_model.compile(loss=_loss, optimizer=_optimizer, metrics=[_metrics])\n",
    "    return loaded_model\n",
    "\n",
    "def is_sentence_a_todo_item(test_string, model):\n",
    "    feature_vec = get_feature_vector(test_string)\n",
    "    predict_matrix = np.array([feature_vec])\n",
    "    test = sequence.pad_sequences(predict_matrix, maxlen=maxlen)\n",
    "    predict_val = model.predict(test[0:1])\n",
    "    #print (model.predict(test[0:1]))\n",
    "    return predict_val[0][0]\n",
    "\n",
    "\n",
    "#print (\"Creating the bag of words...\\n\")\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "# I can probably use the preprocessor to do the taging transformations?  \n",
    "# We can try using the tfidvectorizer that has the advantage of emphasizing \n",
    "# the most important words for a given document. \n",
    "# https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "# https://stackoverflow.com/questions/20132070/using-sklearns-tfidfvectorizer-transform\n",
    "#vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                             tokenizer = None,    \\\n",
    "#                             preprocessor = None, \\\n",
    "#                             stop_words = None,   \\\n",
    "#                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "#train_data_features = vectorizer.fit_transform(training_tags)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "#train_data_features = train_data_features.toarray()\n",
    "\n",
    "\n",
    "#print (\"Creating the bag of words...\\n\")\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "# I can probably use the preprocessor to do the taging transformations?  \n",
    "# We can try using the tfidvectorizer that has the advantage of emphasizing \n",
    "# the most important words for a given document. \n",
    "# https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "# https://stackoverflow.com/questions/20132070/using-sklearns-tfidfvectorizer-transform\n",
    "#vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                             tokenizer = None,    \\\n",
    "#                             preprocessor = None, \\\n",
    "#                             stop_words = None,   \\\n",
    "#                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "#train_data_features = vectorizer.fit_transform(training_tags)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "#train_data_features = train_data_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring this line: (Still going along with the practical joke) You know, I was wondering\n",
      "Ignoring this line: (Jokingly hits the doctor) Whose tonsils grow back? (Laughs)\n",
      "Ignoring this line: Guess whose birthday's comin' up soon?\n",
      "Ignoring this line: Elaine, you know, I was watching you tonight, and I realized something\n",
      "Ignoring this line: Whose pen?\n",
      "Ignoring this line: 'cos the other night , you know, I was sleeping with Marion I rolled over and I cut her ankle with my big toe\n",
      "8568 train sequences\n",
      "952 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (8568, 8)\n",
      "x_test shape: (952, 8)\n",
      "Train on 8568 samples, validate on 952 samples\n",
      "Epoch 1/15\n",
      "8568/8568 [==============================] - 7s 844us/step - loss: 0.4103 - acc: 0.8327 - val_loss: 0.3593 - val_acc: 0.8550\n",
      "Epoch 2/15\n",
      "8568/8568 [==============================] - 6s 680us/step - loss: 0.3415 - acc: 0.8606 - val_loss: 0.3185 - val_acc: 0.8792\n",
      "Epoch 3/15\n",
      "8568/8568 [==============================] - 6s 683us/step - loss: 0.3150 - acc: 0.8737 - val_loss: 0.2933 - val_acc: 0.8855\n",
      "Epoch 4/15\n",
      "8568/8568 [==============================] - 6s 666us/step - loss: 0.2972 - acc: 0.8819 - val_loss: 0.3007 - val_acc: 0.8845\n",
      "Epoch 5/15\n",
      "8568/8568 [==============================] - 6s 659us/step - loss: 0.2836 - acc: 0.8873 - val_loss: 0.2922 - val_acc: 0.8918\n",
      "Epoch 6/15\n",
      "8568/8568 [==============================] - 6s 669us/step - loss: 0.2848 - acc: 0.8883 - val_loss: 0.3092 - val_acc: 0.8750\n",
      "Epoch 7/15\n",
      "8568/8568 [==============================] - 6s 675us/step - loss: 0.2672 - acc: 0.8937 - val_loss: 0.2626 - val_acc: 0.9065\n",
      "Epoch 8/15\n",
      "8568/8568 [==============================] - 6s 672us/step - loss: 0.2598 - acc: 0.8950 - val_loss: 0.2918 - val_acc: 0.8887\n",
      "Epoch 9/15\n",
      "8568/8568 [==============================] - 6s 679us/step - loss: 0.2534 - acc: 0.8982 - val_loss: 0.2986 - val_acc: 0.8939\n",
      "Epoch 10/15\n",
      "8568/8568 [==============================] - 6s 664us/step - loss: 0.2459 - acc: 0.9016 - val_loss: 0.2833 - val_acc: 0.9002\n",
      "Epoch 11/15\n",
      "8568/8568 [==============================] - 6s 671us/step - loss: 0.2425 - acc: 0.9011 - val_loss: 0.2753 - val_acc: 0.8908\n",
      "Epoch 12/15\n",
      "8568/8568 [==============================] - 6s 699us/step - loss: 0.2333 - acc: 0.9037 - val_loss: 0.2927 - val_acc: 0.9034\n",
      "Epoch 13/15\n",
      "8568/8568 [==============================] - 6s 667us/step - loss: 0.2345 - acc: 0.9051 - val_loss: 0.2724 - val_acc: 0.9086\n",
      "Epoch 14/15\n",
      "8568/8568 [==============================] - 6s 675us/step - loss: 0.2308 - acc: 0.9076 - val_loss: 0.2807 - val_acc: 0.8960\n",
      "Epoch 15/15\n",
      "8568/8568 [==============================] - 6s 668us/step - loss: 0.2211 - acc: 0.9107 - val_loss: 0.2879 - val_acc: 0.9013\n",
      "Test score 0.2879134565591812\n",
      "Test accuracy 0.90126050470256\n"
     ]
    }
   ],
   "source": [
    "create_cnn_todo_item_recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from disk... /home/t-judai/model_hack.json\n",
      "Loading the weights from disk... /home/t-judai/model_hack_weights.h5\n",
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/t-judai/model_hack.json\"\n",
    "weights_path = \"/home/t-judai/model_hack_weights.h5\"\n",
    "model = load_cnn_todo_reco_model_from_path(model_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test_string = \"talk to humans\"\n",
    "result = is_sentence_a_todo_item(test_string, model)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "from pandas.io.json import json_normalize\n",
    "import validators\n",
    "\n",
    "import os\n",
    "#os.chdir('C:\\\\Users\\\\ggarcia\\\\Desktop\\\\ClusteringTimeline')\n",
    "\n",
    "#from html_analyzer import GetPageContentText\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# In Python, searching a set is much faster than searching\n",
    "#  a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "allowed_tags_dict = {'NN': True, 'NNS': True, 'JJR': True, 'JJS': True, \n",
    "                     'JJ': True, 'VB': True}\n",
    "\n",
    "only_nouns_tags_dict = {'NN': True, 'NNS': True}\n",
    "\n",
    "document = \"I have a new idea for a metallic dog that has robotic legs so it can roam around on its own. \\\n",
    "            The dog would be able to walk on planets, and since it is pretty small, \\\n",
    "            it can get into places that humans normally could not.\"\n",
    "document1 = \"I have a new idea for a metallic dog that can talk to humans.\"\n",
    "document2 = \"I have a new idea for rainbows and butterflies eating chocolate on the beach and watching the sunset at night\"\n",
    "def clean_document(document):\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", document) \n",
    "    # Remove stop words\n",
    "    stop_free = \" \".join([i for i in letters_only.lower().split() if i not in stops])\n",
    "    # Remove punctionation\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    # Nomalize the words to the base form\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    \n",
    "    # Drop every word with a tag not included in our allowed tags dictionary\n",
    "    meaningful_words_list = []\n",
    "    tags = pos_tag(normalized.split())\n",
    "    for tag in tags:\n",
    "        #We drop long words  (most of the time they are garbage from the html)\n",
    "        if tag[1] in allowed_tags_dict and len(tag[0]) < 10:\n",
    "            if tag[1] in only_nouns_tags_dict and len(tag[0]) < 10:\n",
    "                meaningful_words_list.append(tag[0])\n",
    "    return meaningful_words_list  \n",
    "\n",
    "\n",
    "#documents = get_document_collection(activities_groups)\n",
    "doc = clean_document(document2)\n",
    "#docs_clean = [clean_document(doc) for doc in documents]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idea', 'rainbow', 'butterfly', 'chocolate', 'beach', 'sunset', 'night']\n"
     ]
    }
   ],
   "source": [
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idea rainbow butterfly chocolate beach sunset night \n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"\"\n",
    "for word in doc:\n",
    "    new_sentence += word + \" \"\n",
    "print (new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (is_sentence_a_todo_item(new_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk import ngrams\n",
    "sentence = \"jump rope that can change lengths depending on the setting\"\n",
    "n = 2\n",
    "\n",
    "\n",
    "#sentence = clean_document(sentence)\n",
    "#sentence = \" \".join(sentence)\n",
    "highest_prob = []\n",
    "\n",
    "def get_summary(sentence):\n",
    "    final_sentence = \"\"\n",
    "    sixgrams = ngrams(sentence.split(), 2) \n",
    "    total_scores = []\n",
    "    total_strings = []\n",
    "    for grams in sixgrams:\n",
    "        string = \" \".join(grams)\n",
    "        score = is_sentence_a_todo_item(string, model)\n",
    "        total_scores.append(score)\n",
    "        total_strings.append(string)\n",
    "        #print (string + \" \" + str(score))\n",
    "\n",
    "    #print (len(total_scores))\n",
    "\n",
    "    i = 1\n",
    "    while i < len(total_scores):\n",
    "        if total_scores[i]> 0.5  and total_scores[i-1] < 0.5:\n",
    "            final_sentence += total_strings[i] + \" \"\n",
    "            i += 2\n",
    "        elif total_scores[i] < 0.5 and total_scores[i-1] > 0.5:\n",
    "            final_sentence += total_strings[i-1] + \" \"\n",
    "            i += 2\n",
    "        elif total_scores[i] > 0.5 and total_scores[i-1] > 0.5:\n",
    "            if total_scores[i] > total_scores[i-1]:\n",
    "                final_sentence += total_strings[i] + \" \"\n",
    "                i += 2\n",
    "            else:\n",
    "                final_sentence += total_strings[i-1] + \" \"\n",
    "                i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    list_words = final_sentence.split()\n",
    "    i = 1\n",
    "    while i <  len(list_words):\n",
    "        if list_words[i] == list_words[i-1]:\n",
    "            list_words.pop(i)\n",
    "            i -= 1\n",
    "        i += 1\n",
    "    connected_sentence = \" \".join(list_words)\n",
    "    #print (\"\")\n",
    "    #print (sentence)\n",
    "    return connected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a toy that changes shape depending on what the child is thinking of playing with\n",
      "a toy that changes depending on the child playing with\n",
      "a toy that changes depending on the child playing with\n"
     ]
    }
   ],
   "source": [
    "def get_max_length_summary(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) <= 10:\n",
    "        return sentence\n",
    "    else:\n",
    "        result = get_summary(sentence)\n",
    "        words_result = result.split() \n",
    "        if len(words_result) > 10:\n",
    "            while len(words_result) > 10:\n",
    "                result = get_summary(result)\n",
    "                words_result = result.split()\n",
    "        return result\n",
    "sentence = \"a toy that changes shape depending on what the child is thinking of playing with\"\n",
    "print (\"\")\n",
    "print (sentence)\n",
    "print (get_summary(sentence))\n",
    "print (get_max_length_summary(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "if \"uh\" in words.words():\n",
    "    print (\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/t-judai/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
